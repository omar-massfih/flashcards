What is a data-intensive application?	An application where bottlenecks are usually data volume/complexity/change rate rather than raw CPU.
Name common building blocks of data-intensive applications.	Databases (storage); caches; search indexes; stream processing; batch processing.
Why don’t app teams typically build a new storage engine from scratch?	Data systems are a successful abstraction; reuse is usually cheaper and safer.
Why are there many different database systems?	Different applications have different requirements and access patterns; tradeoffs differ.
Why use the umbrella term “data systems”?	Tool boundaries blur; apps increasingly combine multiple tools into composite systems.
In a composite data system, what keeps caches/indexes in sync?	Application code often updates/invalidates external caches and indexes to match the primary store.
What does a service API typically do when multiple tools are combined behind it?	Hides implementation details while exposing the guarantees the composite system provides.
When you stitch multiple data tools together, what new role do you take on?	You become a data system designer (not just an application developer).
What 3 cross-cutting concerns does DDIA Ch. 1 emphasize?	Reliability; scalability; maintainability.
Define reliability (DDIA framing).	Continuing to work correctly (correct function at required performance) even when things go wrong.
Define scalability (DDIA framing).	Reasonable ways to handle growth in load/data/complexity; add resources while keeping performance acceptable.
Define maintainability (DDIA framing).	Enable teams to work productively over time while maintaining and evolving the system.
What is a fault vs a failure (DDIA)?	Fault: a component deviates from spec; failure: the system as a whole stops providing required service.
What is fault tolerance (resilience)?	Mechanisms that prevent certain faults from turning into user-visible failures.
Why can deliberate fault injection improve reliability?	It exercises fault-handling paths so poor error handling is discovered before real incidents.
Name three broad categories of faults discussed in DDIA Ch. 1.	Hardware faults; software errors; human errors.
Why are hardware faults often treated as random/uncorrelated?	One disk/host failing usually doesn’t imply others fail at the same time (weak correlations aside).
How does redundancy help with hardware faults?	Redundant components can take over while broken parts are replaced, reducing downtime.
Why do large fleets increase the rate of hardware faults you must handle?	More machines means failures happen more frequently in absolute terms.
Why do some cloud environments push you toward software fault tolerance?	Instances can become unavailable without warning; designs prioritize elasticity over single-node reliability.
What operational advantage comes with tolerating machine loss?	Rolling upgrades/patching node-by-node without taking the whole system down.
What makes systematic software faults especially dangerous?	They are correlated across nodes and can trigger widespread failures.
Give examples of systematic software faults (DDIA Ch. 1).	Bug on bad input; runaway resource usage; dependency slowdown/corruption; cascading failures.
Why can software bugs lie dormant for a long time?	They’re triggered only under unusual circumstances when hidden assumptions break.
Name practical techniques to reduce impact of software faults.	Thorough testing; process isolation; crash/restart; monitoring/analysis in prod; careful design of assumptions.
What is an example of a runtime self-check for correctness?	Continuously check an invariant/guarantee and alert on discrepancies (e.g., message counts in/out).
Why are human errors a major reliability risk?	Operators and developers make mistakes; config/change errors are common causes of outages.
How can interfaces reduce human error?	Good abstractions/APIs/admin UIs make the right thing easy and discourage risky actions.
Why can overly restrictive operational interfaces backfire?	People work around them, negating safety benefits.
What is the purpose of non-production sandbox environments?	Safe experimentation with realistic conditions without impacting real users.
Name techniques for fast recovery from human errors.	Roll back config quickly; gradual rollout; recompute data; tooling for recovery.
Why is monitoring/telemetry important for reliability?	Early warning signals; validate assumptions/constraints; faster diagnosis during incidents.
When might teams consciously sacrifice reliability?	To reduce dev/ops cost (e.g., prototypes) but it should be an explicit, conscious tradeoff.
Why is “X is scalable” not meaningful by itself?	Scalability depends on how load grows and what resources/architecture changes you can make.
What are load parameters?	Quantitative measures describing load (e.g., RPS, read/write ratio, active users, cache hit rate).
Why do the “right” load parameters depend on the system?	They must reflect architecture, access patterns, and what bottlenecks dominate (avg vs extremes).
What is fan-out in scalability discussions?	The amount of downstream work/calls/writes triggered by one incoming request (one-to-many effects).
What is the tradeoff between “compute on read” vs “compute on write”?	Compute on read: cheaper writes, expensive reads; compute on write: more work at write time, faster reads.
Why can a hybrid approach help with fan-out-heavy workloads?	Eager fan-out for typical cases; handle extreme fan-out cases differently to keep performance predictable.
In batch systems, what performance metric is commonly emphasized?	Throughput (records/sec) or job runtime for a dataset.
In online systems, what performance metric is commonly emphasized?	Response time (client-visible time from request to response).
What is the difference between response time and latency?	Response time includes service time plus network and queueing delays; latency is time waiting to be served.
Why is response time best treated as a distribution?	Requests vary; outliers matter; a single average hides user experience variance.
Why can the mean be misleading for “typical” response time?	Skew/outliers distort it; it doesn’t show how many users experience slow requests.
What is the median response time (p50)?	The 50th percentile; half of requests are faster and half are slower.
What does p95 (or p99/p999) response time mean?	A threshold where 95% (or 99%/99.9%) of requests are faster than that value.
What are tail latencies?	High-percentile response times (e.g., p99/p999) representing the slow tail/outliers.
Why can tail latencies matter disproportionately?	A few slow requests can dominate user experience and multi-call request latency.
What is head-of-line blocking?	A small number of slow requests block later requests in queues, inflating their response times.
Why measure response times from the client side?	Queueing and network delays contribute to what users see; server-side timing can miss them.
What load-test client behavior can skew latency measurements?	Closed-loop tests (wait for response before next request) keep queues shorter than reality.
What is tail latency amplification?	If a request fans out to multiple backends, overall latency is gated by the slowest sub-request.
Why is averaging percentiles across machines/time mathematically wrong?	Percentiles don’t aggregate by averaging; you must combine distributions (e.g., merge histograms).
What are two high-level scaling approaches?	Vertical scaling (scale up) and horizontal scaling (scale out across machines).
What is a shared-nothing architecture?	Horizontal scaling by distributing load/state across independent machines (no shared central bottleneck).
What is elasticity vs manual scaling?	Elastic systems auto-add resources; manual scaling relies on human capacity planning.
Why is scaling stateless services easier than scaling stateful data systems?	State distribution/consistency adds complexity; stateless replicas can be load-balanced more directly.
What 3 principles does DDIA Ch. 1 highlight for maintainability?	Operability; simplicity; evolvability.
Define operability.	Make it easy to keep the system running smoothly in production (monitor, deploy, diagnose, maintain).
Define simplicity (DDIA maintainability principle).	Reduce accidental complexity so engineers can understand and reason about the system.
Define evolvability.	Make it easy to change/adapt the system for new requirements over time.
What is accidental complexity?	Complexity from implementation choices, not inherent to the user’s problem.
Why is abstraction a key tool for simplicity?	It hides implementation details behind a clean interface; reuse concentrates quality improvements.
Give examples of abstractions that hide complexity.	High-level languages hide machine details; SQL hides storage/concurrency/crash recovery details.
